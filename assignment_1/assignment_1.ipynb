{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<body>\n",
    "    <center> \n",
    "        <h1><u>Assignment 1</u></h1>\n",
    "        <h3> Quick intro + checking code works on your system </h3>\n",
    "    </center>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Outcomes: The goal of this assignment is two-fold:\n",
    "\n",
    "- This code-base is designed to be easily extended for different research projects. Running this notebook to the end will ensure that the code runs on your system, and that you are set-up to start playing with machine learning code.\n",
    "- This notebook has one complete application: training a CNN classifier to predict the digit in MNIST Images. The code is written to familiarize you to a typical machine learning pipeline, and to the building blocks of code used to do ML. So, read on! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please specify your Name, Email ID and forked repository url here:\n",
    "- Name:\n",
    "- Email:\n",
    "- Link to your forked github repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General libraries useful for python ###\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your github directory is :/net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI\n"
     ]
    }
   ],
   "source": [
    "### Finding where you clone your repo, so that code upstream paths can be specified programmatically ####\n",
    "work_dir = os.getcwd()\n",
    "git_dir = '/'.join(work_dir.split('/')[:-1])\n",
    "print('Your github directory is :%s'%git_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Libraries for visualizing our results and data ###\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Import PyTorch and its components ###\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's load our flexible code-base which you will build on for your research projects in future assignments.\n",
    "\n",
    "Above we have imported modules (libraries for those familiar to programming languages other than python). These modules are of two kinds - (1) inbuilt python modules like `os`, `sys`, `random`, or (2) ones which we installed using conda (ex. `torch`).\n",
    "\n",
    "Below we will be importing our own written code which resides in the `res` folder in your github directory. This is structured to be easily expandable for different machine learning projects. Suppose that you want to do a project on object detection. You can easily add a few files to the sub-folders within `res`, and this script will then be flexibly do detection instead of classication (which is presented here). Expanding on this codebase will be the main subject matter of Assignment 2. For now, let's continue with importing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models are being loaded from: /net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/res/models\n",
      "Loaders are being loaded from: /net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/res/loader\n"
     ]
    }
   ],
   "source": [
    "### Making helper code under the folder res available. This includes loaders, models, etc. ###\n",
    "sys.path.append('%s/res/'%git_dir)\n",
    "from models.models import get_model\n",
    "from loader.loader import get_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See those paths printed above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`res/models` holds different model files. So, if you want to load ResNet architecture or a transformers architecture, they will reside there as separate files. \n",
    "\n",
    "Similarly, `res/loader` holds programs which are designed to load different types of data. For example, you may want to load data differently for object classification and detection. For classification each image will have only a numerical label corresponding to its category. For detection, the labels for the same image would contain bounding boxes for different objects and the type of the object in the box. \n",
    "\n",
    "So, to expand further you will be adding files to the folders above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Setting up Weights and Biases for tracking your experiments. ###\n",
    "\n",
    "We have Weights and Biases (wandb.ai) integrated into the code for easy visualization of results and for tracking performance. `Please make an account at wandb.ai, and follow the steps to login to your account!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mspandanmadan\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying settings/hyperparameters for our code below ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_config = {}\n",
    "wandb_config['batch_size'] = 10\n",
    "wandb_config['base_lr'] = 0.01\n",
    "wandb_config['model_arch'] = 'CustomCNN'\n",
    "wandb_config['num_classes'] = 10\n",
    "wandb_config['run_name'] = 'assignment_1'\n",
    "\n",
    "### If you are using a CPU, please set wandb_config['use_gpu'] = 0 below. However, if you are using a GPU, leave it unchanged ####\n",
    "wandb_config['use_gpu'] = 1\n",
    "\n",
    "wandb_config['num_epochs'] = 2\n",
    "wandb_config['git_dir'] = git_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing above, different experiments can be run. For example, you can specify which model architecture to load, which dataset you will be loading, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common task many of you will be doing in your projects will be running a script on a new dataset. In PyTorch this is done using data loaders, and it is extremely important to understand this works. In next assignment, you will be writing your own dataloader. For now, we only expose you to basic data loading which for the MNIST dataset for which PyTorch provides easy functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load MNIST. The first time you run it, the dataset gets downloaded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Transforms tell PyTorch how to pre-process your data. Recall that images are stored with values between 0-255 usually. One very common pre-processing for images is to normalize to be 0 mean and 1 standard deviation. This pre-processing makes the task easier for neural networks. There are many, many kinds of normalization in deep learning, the most basic one being those imposed on the image data while loading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {}\n",
    "data_transforms['train'] = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))])\n",
    "\n",
    "data_transforms['test'] = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchvision.datasets.MNIST` allows you to load MNIST data. In future, we will be using our own `get_loader` function from above to load custom data. Notice that data_transforms are passed as argument while loading the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e1ccacbe964043a6fb780b06a5f8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/datasets/MNIST/raw/train-images-idx3-ubyte.gz to /net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/datasets/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54cf32ea71f4c52b56cb2e2fc2839b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/datasets/MNIST/raw/train-labels-idx1-ubyte.gz to /net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/datasets/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b54a2c2129416c9cd3554697567072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to /net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/datasets/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76be03fc31e54d1c869b1b5e289a9b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to /net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/datasets/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset = {}\n",
    "mnist_dataset['train'] = torchvision.datasets.MNIST('%s/datasets'%wandb_config['git_dir'], train = True, download = True, transform = data_transforms['train'])\n",
    "mnist_dataset['test'] = torchvision.datasets.MNIST('%s/datasets'%wandb_config['git_dir'], train = False, download = True, transform = data_transforms['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset vs Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most deep learning datasets are huge. Can be as large as million data points. We want to keep our GPUs free to store intermediate calculations for neural networks, like gradients. We would not be able to load a million samples into the GPU (or even CPU) and do forward or backward passes on the network. \n",
    "\n",
    "So, samples are loaded in batches, and this method of gradient descent is called mini-batch gradient descent. `torch.utils.data.DataLoader` allows you to specify a pytorch dataset, and makes it easy to loop over it in batches. So, we leverage this to create a data loader from our above loaded MNIST dataset. \n",
    "\n",
    "The dataset itself only contains lists of where to find the inputs and outputs i.e. paths. The data loader defines the logic on loading this information into the GPU/CPU and so it can be passed into the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders = {}\n",
    "data_loaders['train'] = torch.utils.data.DataLoader(mnist_dataset['train'], batch_size = wandb_config['batch_size'], shuffle = True)\n",
    "data_loaders['test'] = torch.utils.data.DataLoader(mnist_dataset['test'], batch_size = wandb_config['batch_size'], shuffle = False)\n",
    "\n",
    "data_sizes = {}\n",
    "data_sizes['train'] = len(mnist_dataset['train'])\n",
    "data_sizes['test'] = len(mnist_dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use the `get_model` functionality to load a CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(wandb_config['model_arch'], wandb_config['num_classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do you know what you are doing?\n",
    "\n",
    "`get_model` is just a function in the file `res/models/models.py`. Stop here, open this file, and see what the function does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9466781b0ae42fe851b61beb5f2875e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Read the function?\\n Click me!', layout=Layout(height='90px', width='auto'), style=ButtonS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36765f222a47440b99e1e37cde5e3226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layout = widgets.Layout(width='auto', height='90px') #set width and height\n",
    "\n",
    "button = widgets.Button(description=\"Read the function?\\n Click me!\", layout=layout)\n",
    "output = widgets.Output()\n",
    "\n",
    "display(button, output)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        print(\"As you can see, the function simply returns an object of the class CustomCNN, which is defined in res/models/CustomCNN.py\")\n",
    "        print(\"This is our neural network model.\")\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below we have the function which trains, tests and returns the best model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(model, criterion, optimizer, dset_loaders, dset_sizes, hyperparameters):\n",
    "    with wandb.init(project=\"HARVAR_BAI\", config=hyperparameters):\n",
    "        if hyperparameters['run_name']:\n",
    "            wandb.run.name = hyperparameters['run_name']\n",
    "        config = wandb.config\n",
    "        best_model = model\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        print(config)\n",
    "        \n",
    "        print(config.num_epochs)\n",
    "        for epoch_num in range(config.num_epochs):\n",
    "            wandb.log({\"Current Epoch\": epoch_num})\n",
    "            model = train_model(model, criterion, optimizer, dset_loaders, dset_sizes, config)\n",
    "            best_acc, best_model = test_model(model, best_acc, best_model, dset_loaders, dset_sizes, config)\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The different steps of the train model function are annotated below inside the function. Read them step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dset_loaders, dset_sizes, configs):\n",
    "    print('Starting training epoch...')\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "\n",
    "    \n",
    "    ### This tells python to track gradients. While testing weights aren't updated hence they are not stored.\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    iters = 0\n",
    "    \n",
    "    \n",
    "    ### We loop over the data loader we created above. Simply using a for loop.\n",
    "    for data in tqdm(dset_loaders['train']):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        ### If you are using a gpu, then script will move the loaded data to the GPU. \n",
    "        ### If you are not using a gpu, ensure that wandb_configs['use_gpu'] is set to False above.\n",
    "        if configs.use_gpu:\n",
    "            inputs = inputs.float().cuda()\n",
    "            labels = labels.long().cuda()\n",
    "        else:\n",
    "            print('WARNING: NOT USING GPU!')\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.long()\n",
    "\n",
    "        \n",
    "        ### We set the gradients to zero, then calculate the outputs, and the loss function. \n",
    "        ### Gradients for this process are automatically calculated by PyTorch.\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "        ### At this point, the program has calculated gradient of loss w.r.t. weights of our NN model.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### optimizer.step() updated the models weights using calculated gradients.\n",
    "        \n",
    "        ### Let's store these and log them using wandb. They will be displayed in a nice online\n",
    "        ### dashboard for you to see.\n",
    "        \n",
    "        iters += 1\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        wandb.log({\"train_running_loss\": running_loss/float(iters*len(labels.data))})\n",
    "        wandb.log({\"train_running_corrects\": running_corrects/float(iters*len(labels.data))})\n",
    "\n",
    "    epoch_loss = float(running_loss) / dset_sizes['train']\n",
    "    epoch_acc = float(running_corrects) / float(dset_sizes['train'])\n",
    "    wandb.log({\"train_accuracy\": epoch_acc})\n",
    "    wandb.log({\"train_loss\": epoch_loss})\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, best_acc, best_model, dset_loaders, dset_sizes, configs):\n",
    "    print('Starting testing epoch...')\n",
    "    model.eval() ### tells pytorch to not store gradients as we won't be updating weights while testing.\n",
    "\n",
    "    running_corrects = 0\n",
    "    iters = 0   \n",
    "    for data in tqdm(dset_loaders['test']):\n",
    "        inputs, labels = data\n",
    "        if configs.use_gpu:\n",
    "            inputs = inputs.float().cuda()\n",
    "            labels = labels.long().cuda()\n",
    "        else:\n",
    "            print('WARNING: NOT USING GPU!')\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.long()\n",
    "\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        \n",
    "        iters += 1\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        wandb.log({\"train_running_corrects\": running_corrects/float(iters*len(labels.data))})\n",
    "\n",
    "\n",
    "    epoch_acc = float(running_corrects) / float(dset_sizes['test'])\n",
    "\n",
    "    wandb.log({\"test_accuracy\": epoch_acc})\n",
    "    \n",
    "    ### Code is very similar to train set. One major difference, we don't update weights. \n",
    "    ### We only check the performance is best so far, if so, we save this model as the best model so far.\n",
    "    \n",
    "    if epoch_acc > best_acc:\n",
    "        best_acc = epoch_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    wandb.log({\"best_accuracy\": best_acc})\n",
    "    \n",
    "    return best_acc, best_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Criterion is simply specifying what loss to use. Here we choose cross entropy loss. \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "### tells what optimizer to use. There are many options, we here choose Adam.\n",
    "### the main difference between optimizers is that they vary in how weights are updated based on calculated gradients.\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr = wandb_config['base_lr'])\n",
    "\n",
    "if wandb_config['use_gpu']:\n",
    "    criterion.cuda()\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating the folder where our models will be saved.\n",
    "if not os.path.isdir(\"%s/saved_models/\"%wandb_config['git_dir']):\n",
    "    os.mkdir(\"%s/saved_models/\"%wandb_config['git_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.15<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">efficient-mountain-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/spandanmadan/HARVAR_BAI\" target=\"_blank\">https://wandb.ai/spandanmadan/HARVAR_BAI</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/spandanmadan/HARVAR_BAI/runs/3le74nwk\" target=\"_blank\">https://wandb.ai/spandanmadan/HARVAR_BAI/runs/3le74nwk</a><br/>\n",
       "                Run data is saved locally in <code>/net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/assignment_1/wandb/run-20210128_203814-3le74nwk</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_epochs': 2, 'use_gpu': 1, 'run_name': 'TEST_1', 'model_arch': 'CustomCNN', 'git_dir': '/net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI', 'batch_size': 10, 'num_classes': 10, 'base_lr': 0.01}\n",
      "2\n",
      "Starting training epoch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4da534b0c9143f99679fb17e21bc028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing epoch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1874714ea454858ad1c0b97c9cd97a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b969c1eecd40259a016f9b109e6d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing epoch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7900ac5abfb44bd18352ad12805897cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18704<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/assignment_1/wandb/run-20210128_203814-3le74nwk/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/assignment_1/wandb/run-20210128_203814-3le74nwk/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>best_accuracy</td><td>0.5548</td></tr><tr><td>Current Epoch</td><td>1</td></tr><tr><td>_timestamp</td><td>1611884406</td></tr><tr><td>_step</td><td>26009</td></tr><tr><td>train_loss</td><td>0.19313</td></tr><tr><td>train_running_loss</td><td>0.19313</td></tr><tr><td>_runtime</td><td>111</td></tr><tr><td>train_accuracy</td><td>0.52985</td></tr><tr><td>test_accuracy</td><td>0.5548</td></tr><tr><td>train_running_corrects</td><td>0.5548</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>best_accuracy</td><td>▁█</td></tr><tr><td>Current Epoch</td><td>▁█</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>train_running_loss</td><td>█▄▃▂▂▂▂▂▂▂▁▁▁▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁█</td></tr><tr><td>test_accuracy</td><td>▁█</td></tr><tr><td>train_running_corrects</td><td>▁▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇▇▇███▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">efficient-mountain-4</strong>: <a href=\"https://wandb.ai/spandanmadan/HARVAR_BAI/runs/3le74nwk\" target=\"_blank\">https://wandb.ai/spandanmadan/HARVAR_BAI/runs/3le74nwk</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Let's run it all, and save the final best model.\n",
    "\n",
    "\n",
    "best_final_model = model_pipeline(model, criterion, optimizer_ft, data_loaders, data_sizes, wandb_config)\n",
    "\n",
    "\n",
    "save_path = '%s/saved_models/%s_final.pt'%(wandb_config['git_dir'], wandb_config['run_name'])\n",
    "with open(save_path,'wb') as F:\n",
    "    torch.save(best_final_model,F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "You just completed your first deep learning program - image classification for MNIST. This wraps up assignment 1. In the next assignment, we will see how you can make changes to above mentioned folders/files to adapt this code-base to your own research project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverables for Assignment 1: \n",
    "\n",
    "### Please run this assignment through to the end, and then make two submissions:\n",
    "\n",
    "- Download this notebook as an HTML file. Click File ---> Download as ---> HTML. Submit this on canvas.\n",
    "- Add, commit and push these changes to your github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bai",
   "language": "python",
   "name": "bai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
